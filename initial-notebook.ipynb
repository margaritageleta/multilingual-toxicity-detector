{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bert-tensorflow in /usr/local/lib/python3.6/dist-packages (1.0.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.14.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.18.4)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas) (1.14.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.11.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2020.6.8)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.46.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.25.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (2.6)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.14.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-tensorflow\n",
    "!pip install pandas\n",
    "!pip install transformers # https://huggingface.co/transformers/installation.html#with-pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2.2.0\nNum GPUs Available:  0\n"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import transformers\n",
    "from tqdm.notebook import tqdm\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "print(tf.version.VERSION)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH =  \"./jigsaw-multilingual-toxic-comment-classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training data from our first competition,\n",
    "# https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data\n",
    "wiki_toxic_comment_data = \"jigsaw-toxic-comment-train.csv\"\n",
    "\n",
    "wiki_toxic_comment_train = pandas.read_csv(os.path.join(DATA_PATH, wiki_toxic_comment_data))\n",
    "wiki_toxic_comment_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):\n",
    "    tokenizer.enable_truncation(max_length = maxlen)\n",
    "    tokenizer.enable_padding(max_length = maxlen)\n",
    "    all_ids = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), chunk_size)):\n",
    "        text_chunk = texts[i:i + chunk_size].tolist()\n",
    "        encs = tokenizer.encode_batch(text_chunk)\n",
    "        all_ids.extend([enc.ids for enc in encs])\n",
    "    \n",
    "    return np.array(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "def set_optimizer(type = 'adam', lr = 1e-5):\n",
    "    if type == 'adam':\n",
    "        return Adam(lr = lr)\n",
    "    elif type == 'adadelta':\n",
    "        return Adadelta(lr = lr)\n",
    "    else: \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "def build_model(transformer, optimizer = set_optimizer('adam'), dropout_prop = 0.35, \n",
    "                loss = 'binary_crossentropy', max_len = 512):\n",
    "    # Instantiate tensor with input word IDs\n",
    "    input_word_ids = Input(shape = (max_len,), dtype = tf.int32, name ='input_word_ids')\n",
    "    \n",
    "    # Forward through transformer network\n",
    "    # Hugginface transformers have multiple outputs, embeddings are at the first one\n",
    "    sequence_output = transformer(input_word_ids)[0]\n",
    "    # We slice the first position, the paper says it is not worse than pooling\n",
    "    cls_token = sequence_output[:, 0, :]\n",
    "    \n",
    "    # Apply dropout to dense layer and final layer to ouput toxicity probability\n",
    "    x = tf.keras.layers.Dropout(dropout_prop)(cls_token)\n",
    "    out = Dense(1, activation = 'sigmoid', name = 'toxicity_prob_head')(x)\n",
    "    \n",
    "    model = Model(inputs = input_word_ids, outputs = out)\n",
    "    model.compile(optimizer, loss = loss, metrics = ['accuracy', tf.keras.metrics.AUC()])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Tokenizer(vocabulary_size=119547, model=BertWordPiece, unk_token=[UNK], sep_token=[SEP], cls_token=[CLS], pad_token=[PAD], mask_token=[MASK], clean_text=True, handle_chinese_chars=True, strip_accents=True, lowercase=False, wordpieces_prefix=##)"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "#tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n",
    "# Save the loaded tokenizer locally\n",
    "#tokenizer.save_pretrained('.')\n",
    "# Reload it with the huggingface tokenizers library\n",
    "fast_tokenizer = BertWordPieceTokenizer('tokenizers/distilbert/vocab.txt', lowercase=False)\n",
    "fast_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and balance toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = pd.read_csv(os.path.join(DATA_PATH, 'jigsaw-toxic-comment-train.csv'))\n",
    "train2 = pd.read_csv(os.path.join(DATA_PATH, 'jigsaw-unintended-bias-train.csv'))\n",
    "train2.toxic = train2.toxic.round().astype(int)\n",
    "\n",
    "valid = pd.read_csv(os.path.join(DATA_PATH, 'validation.csv'))\n",
    "test = pd.read_csv(os.path.join(DATA_PATH, 'test.csv'))\n",
    "sub = pd.read_csv(os.path.join(DATA_PATH, 'sample_submission.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train1 toxic comments 21384\n",
      "Train1 non-toxic comments 202165\n",
      "Proportion train1 toxic comments 9.565688059441108\n",
      "Proportion train2 non-toxic comments 90.4343119405589\n",
      "\n",
      "Train2 toxic comments 112226\n",
      "Train2 non-toxic comments 1789968\n",
      "Proportion train2 toxic comments 5.899818840770185\n",
      "Proportion train2 non-toxic comments 94.1001811592298\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train1 toxic comments {len(train1[['comment_text', 'toxic']].query('toxic==1'))}\")\n",
    "print(f\"Train1 non-toxic comments {len(train1[['comment_text', 'toxic']].query('toxic==0'))}\")\n",
    "print(f\"Proportion train1 toxic comments {len(train1[['comment_text', 'toxic']].query('toxic==1')) / len(train1[['comment_text', 'toxic']]) * 100}\")\n",
    "print(f\"Proportion train2 non-toxic comments {len(train1[['comment_text', 'toxic']].query('toxic==0')) / len(train1[['comment_text', 'toxic']]) * 100}\\n\")\n",
    "\n",
    "print(f\"Train2 toxic comments {len(train2[['comment_text', 'toxic']].query('toxic==1'))}\")\n",
    "print(f\"Train2 non-toxic comments {len(train2[['comment_text', 'toxic']].query('toxic==0'))}\")\n",
    "print(f\"Proportion train2 toxic comments {len(train2[['comment_text', 'toxic']].query('toxic==1')) / len(train2[['comment_text', 'toxic']]) * 100}\")\n",
    "print(f\"Proportion train2 non-toxic comments {len(train2[['comment_text', 'toxic']].query('toxic==0')) / len(train2[['comment_text', 'toxic']]) * 100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample the train dataframe to 50% - 50%\n",
    "train = pd.concat([\n",
    "    train1[['comment_text', 'toxic']].query('toxic==1'),\n",
    "    train1[['comment_text', 'toxic']].query('toxic==0').sample(n = 25000, random_state = 0),\n",
    "    train2[['comment_text', 'toxic']].query('toxic==1'),\n",
    "    train2[['comment_text', 'toxic']].query('toxic==0').sample(n = 150000, random_state = 0)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "308610\n",
      "Final dataset toxic proportion 43.294125271378114\n",
      "Final dataset non-toxic proportion 56.705874728621886\n"
     ]
    }
   ],
   "source": [
    "print(len(train))\n",
    "print(f\"Final dataset toxic proportion {len(train[['comment_text', 'toxic']].query('toxic==1')) / len(train[['comment_text', 'toxic']]) * 100}\")\n",
    "print(f\"Final dataset non-toxic proportion {len(train[['comment_text', 'toxic']].query('toxic==0')) / len(train[['comment_text', 'toxic']]) * 100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9be862db64c4c8092708a402647d80f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1898.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a564ea7d97194694bb7664c610be8546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6489bd661fe4e98937c5136a4769891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=250.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 16 \n",
    "MAX_LEN = 192\n",
    "\n",
    "x_train = fast_encode(train.comment_text.astype(str), fast_tokenizer, maxlen = MAX_LEN)\n",
    "x_valid = fast_encode(valid.comment_text.astype(str), fast_tokenizer, maxlen = MAX_LEN)\n",
    "x_test = fast_encode(test.content.astype(str), fast_tokenizer, maxlen = MAX_LEN)\n",
    "\n",
    "y_train = train.toxic.values\n",
    "y_valid = valid.toxic.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((x_train, y_train))\n",
    "    .repeat()\n",
    "    .shuffle(2048)\n",
    "    .batch(BATCH_SIZE)\n",
    ")\n",
    "\n",
    "valid_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((x_valid, y_valid))\n",
    "    .batch(BATCH_SIZE)\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "test_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices(x_test)\n",
    "    .batch(BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf5a9b88865e4078bec7630864ad9080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea11e66bdb8648e0a4b042ca354ae29e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=910749124.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_word_ids (InputLayer)  [(None, 192)]             0         \n",
      "_________________________________________________________________\n",
      "tf_distil_bert_model (TFDist ((None, 192, 768),)       134734080 \n",
      "_________________________________________________________________\n",
      "tf_op_layer_strided_slice (T [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 769       \n",
      "=================================================================\n",
      "Total params: 134,734,849\n",
      "Trainable params: 134,734,849\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transformer_layer = (\n",
    "        transformers.TFDistilBertModel\n",
    "        .from_pretrained('distilbert-base-multilingual-cased')\n",
    "    )\n",
    "model = build_model(transformer_layer, max_len=MAX_LEN)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 30360 steps, validate for 500 steps\n",
      "Epoch 1/3\n",
      " 7923/30360 [======>.......................] - ETA: 2:39:12 - loss: 0.1123 - accuracy: 0.9564"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17744/30360 [================>.............] - ETA: 1:29:23 - loss: 0.0909 - accuracy: 0.9636"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30360/30360 [==============================] - 12968s 427ms/step - loss: 0.0573 - accuracy: 0.9772 - val_loss: 0.5147 - val_accuracy: 0.8506\n",
      "Epoch 2/3\n",
      "18656/30360 [=================>............] - ETA: 1:22:53 - loss: 0.0652 - accuracy: 0.9735"
     ]
    }
   ],
   "source": [
    "n_steps = x_train.shape[0] // BATCH_SIZE\n",
    "train_history = model.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=n_steps,\n",
    "    validation_data=valid_dataset,\n",
    "    epochs=EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = x_valid.shape[0] // BATCH_SIZE\n",
    "train_history_2 = model.fit(\n",
    "    valid_dataset.repeat(),\n",
    "    steps_per_epoch=n_steps,\n",
    "    epochs=EPOCHS*2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['toxic'] = model.predict(test_dataset, verbose=1)\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p models\n",
    "model.save(f'models/distilbert_batch{BATCH_SIZE}_epochs{EPOCHS}_maxlen{MAX_LEN}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('cpu:0'):\n",
    "    new_model = tf.keras.models.load_model(f'models/CPU_distilbert_batch16_epochs3_maxlen192')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nTotal params: 0\nTrainable params: 0\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toxic(text):\n",
    "    word = pd.DataFrame(data = {'content': [text]})\n",
    "    print(word)\n",
    "    word_test = fast_encode(word.content.astype(str), fast_tokenizer, maxlen = 192)\n",
    "    print(word_test)\n",
    "    word_test_dataset = (\n",
    "        tf.data.Dataset\n",
    "        .from_tensor_slices(word_test)\n",
    "        .batch(16)\n",
    "    )\n",
    "    print(word_test_dataset)\n",
    "    pred = new_model.predict(word_test_dataset, verbose = 1)\n",
    "    print(pred)\n",
    "    return f'Toxic {np.round(pred[0][0] * 100, 3)}%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "content\n0    Hola\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))",
      "text/html": "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n<p>\n  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n  that the widgets JavaScript is still loading. If this message persists, it\n  likely means that the widgets JavaScript library is either not installed or\n  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n  Widgets Documentation</a> for setup instructions.\n</p>\n<p>\n  If you're reading this message in another frontend (for example, a static\n  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n  it may mean that your frontend doesn't currently support widgets.\n</p>\n",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "80c6146774c940cf977af254595435c0"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n[[  101 20220 10330   102     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0]]\n<BatchDataset shapes: (None, 192), types: tf.int64>\n"
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /Users/Margi7/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:1147 predict_function  *\n        outputs = self.distribute_strategy.run(\n    /Users/Margi7/anaconda3/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /Users/Margi7/anaconda3/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /Users/Margi7/anaconda3/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /Users/Margi7/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:1122 predict_step  **\n        return self(x, training=False)\n    /Users/Margi7/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py:927 __call__\n        outputs = call_fn(cast_inputs, *args, **kwargs)\n    /Users/Margi7/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model/utils.py:71 return_outputs_and_add_losses\n        outputs, losses = fn(inputs, *args, **kwargs)\n    /Users/Margi7/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model/utils.py:170 wrap_with_training_arg\n        lambda: replace_training_and_call(False))\n    /Users/Margi7/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/utils/tf_utils.py:65 smart_cond\n        pred, true_fn=true_fn, false_fn=false_fn, name=name)\n    /Users/Margi7/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/smart_cond.py:56 smart_cond\n        return false_fn()\n    /Users/Margi7/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model/utils.py:170 <lambda>\n        lambda: replace_training_and_call(False))\n    /Users/Margi7/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model/utils.py:165 replace_training_and_call\n        return wrapped_call(*args, **kwargs)\n    /Users/Margi7/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py:580 __call__\n        result = self._call(*args, **kwds)\n    /Users/Margi7/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py:627 _call\n        self._initialize(args, kwds, add_initializers_to=initializers)\n    /Users/Margi7/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py:506 _initialize\n        *args, **kwds))\n    /Users/Margi7/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/function.py:2446 _get_concrete_function_internal_garbage_collected\n        graph_function, _, _ = self._maybe_define_function(args, kwargs)\n    /Users/Margi7/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/function.py:2777 _maybe_define_function\n        graph_function = self._create_graph_function(args, kwargs)\n    /Users/Margi7/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/function.py:2667 _create_graph_function\n        capture_by_value=self._capture_by_value),\n    /Users/Margi7/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py:981 func_graph_from_py_func\n        func_outputs = python_func(*func_args, **func_kwargs)\n    /Users/Margi7/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py:441 wrapped_fn\n        return weak_wrapped_fn().__wrapped__(*args, **kwds)\n    /Users/Margi7/anaconda3/lib/python3.6/site-packages/tensorflow/python/saved_model/function_deserialization.py:261 restored_function_body\n        \"\\n\\n\".join(signature_descriptions)))\n\n    ValueError: Could not find matching function to call loaded from the SavedModel. Got:\n      Positional arguments (3 total):\n        * Tensor(\"inputs:0\", shape=(None, 192), dtype=int64)\n        * False\n        * None\n      Keyword arguments: {}\n    \n    Expected these arguments to match one of the following 4 option(s):\n    \n    Option 1:\n      Positional arguments (3 total):\n        * TensorSpec(shape=(None, 192), dtype=tf.int32, name='input_word_ids')\n        * True\n        * None\n      Keyword arguments: {}\n    \n    Option 2:\n      Positional arguments (3 total):\n        * TensorSpec(shape=(None, 192), dtype=tf.int32, name='inputs')\n        * False\n        * None\n      Keyword arguments: {}\n    \n    Option 3:\n      Positional arguments (3 total):\n        * TensorSpec(shape=(None, 192), dtype=tf.int32, name='input_word_ids')\n        * False\n        * None\n      Keyword arguments: {}\n    \n    Option 4:\n      Positional arguments (3 total):\n        * TensorSpec(shape=(None, 192), dtype=tf.int32, name='inputs')\n        * True\n        * None\n      Keyword arguments: {}\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-cdfda48f58b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtoxic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hola\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-ffbca8c73b50>\u001b[0m in \u001b[0;36mtoxic\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     10\u001b[0m     )\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_test_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_test_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34mf'Toxic {np.round(pred[0][0] * 100, 3)}%'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m     87\u001b[0m           method.__name__))\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1266\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1268\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1269\u001b[0m             \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1270\u001b[0m             \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    625\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    504\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    505\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 506\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2444\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2445\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2446\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2447\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2777\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2778\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2665\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2666\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2667\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2668\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2669\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /Users/Margi7/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:1147 predict_function  *\n        outputs = self.distribute_strategy.run(\n    /Users/Margi7/anaconda3/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /Users/Margi7/anaconda3/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /Users/Margi7/anaconda3/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /Users/Margi7/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:1122 predict_step  **\n        return self(x, training=False)\n    /Users/Margi7/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py:927 __call__\n        outputs = call_fn(cast_inputs, *args, **kwargs)\n    /Users/Margi7/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model/utils.py:71 return_outputs_and_add_losses\n        outputs, losses = fn(inputs, *args, **kwargs)\n    /Users/Margi7/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model/utils.py:170 wrap_with_training_arg\n        lambda: replace_training_and_call(False))\n    /Users/Margi7/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/utils/tf_utils.py:65 smart_cond\n        pred, true_fn=true_fn, false_fn=false_fn, name=name)\n    /Users/Margi7/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/smart_cond.py:56 smart_cond\n        return false_fn()\n    /Users/Margi7/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model/utils.py:170 <lambda>\n        lambda: replace_training_and_call(False))\n    /Users/Margi7/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model/utils.py:165 replace_training_and_call\n        return wrapped_call(*args, **kwargs)\n    /Users/Margi7/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py:580 __call__\n        result = self._call(*args, **kwds)\n    /Users/Margi7/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py:627 _call\n        self._initialize(args, kwds, add_initializers_to=initializers)\n    /Users/Margi7/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py:506 _initialize\n        *args, **kwds))\n    /Users/Margi7/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/function.py:2446 _get_concrete_function_internal_garbage_collected\n        graph_function, _, _ = self._maybe_define_function(args, kwargs)\n    /Users/Margi7/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/function.py:2777 _maybe_define_function\n        graph_function = self._create_graph_function(args, kwargs)\n    /Users/Margi7/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/function.py:2667 _create_graph_function\n        capture_by_value=self._capture_by_value),\n    /Users/Margi7/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py:981 func_graph_from_py_func\n        func_outputs = python_func(*func_args, **func_kwargs)\n    /Users/Margi7/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py:441 wrapped_fn\n        return weak_wrapped_fn().__wrapped__(*args, **kwds)\n    /Users/Margi7/anaconda3/lib/python3.6/site-packages/tensorflow/python/saved_model/function_deserialization.py:261 restored_function_body\n        \"\\n\\n\".join(signature_descriptions)))\n\n    ValueError: Could not find matching function to call loaded from the SavedModel. Got:\n      Positional arguments (3 total):\n        * Tensor(\"inputs:0\", shape=(None, 192), dtype=int64)\n        * False\n        * None\n      Keyword arguments: {}\n    \n    Expected these arguments to match one of the following 4 option(s):\n    \n    Option 1:\n      Positional arguments (3 total):\n        * TensorSpec(shape=(None, 192), dtype=tf.int32, name='input_word_ids')\n        * True\n        * None\n      Keyword arguments: {}\n    \n    Option 2:\n      Positional arguments (3 total):\n        * TensorSpec(shape=(None, 192), dtype=tf.int32, name='inputs')\n        * False\n        * None\n      Keyword arguments: {}\n    \n    Option 3:\n      Positional arguments (3 total):\n        * TensorSpec(shape=(None, 192), dtype=tf.int32, name='input_word_ids')\n        * False\n        * None\n      Keyword arguments: {}\n    \n    Option 4:\n      Positional arguments (3 total):\n        * TensorSpec(shape=(None, 192), dtype=tf.int32, name='inputs')\n        * True\n        * None\n      Keyword arguments: {}\n"
     ]
    }
   ],
   "source": [
    "toxic(\"Hola\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Doctor Who adlı viki başlığına 12. doctor olar...</td>\n",
       "      <td>tr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Вполне возможно, но я пока не вижу необходимо...</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Quindi tu sei uno di quelli   conservativi  , ...</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Malesef gerçekleştirilmedi ancak şöyle bir şey...</td>\n",
       "      <td>tr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>:Resim:Seldabagcan.jpg resminde kaynak sorunu ...</td>\n",
       "      <td>tr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63807</th>\n",
       "      <td>63807</td>\n",
       "      <td>No, non risponderò, come preannunciato. Prefer...</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63808</th>\n",
       "      <td>63808</td>\n",
       "      <td>Ciao, I tecnici della Wikimedia Foundation sta...</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63809</th>\n",
       "      <td>63809</td>\n",
       "      <td>innnazitutto ti ringrazio per i ringraziamenti...</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63810</th>\n",
       "      <td>63810</td>\n",
       "      <td>Kaç olumlu oy gerekiyor? Şu an 7 oldu.  Hayır...</td>\n",
       "      <td>tr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63811</th>\n",
       "      <td>63811</td>\n",
       "      <td>Te pido disculpas. La verdad es que no me per...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63812 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                            content lang\n",
       "0          0  Doctor Who adlı viki başlığına 12. doctor olar...   tr\n",
       "1          1   Вполне возможно, но я пока не вижу необходимо...   ru\n",
       "2          2  Quindi tu sei uno di quelli   conservativi  , ...   it\n",
       "3          3  Malesef gerçekleştirilmedi ancak şöyle bir şey...   tr\n",
       "4          4  :Resim:Seldabagcan.jpg resminde kaynak sorunu ...   tr\n",
       "...      ...                                                ...  ...\n",
       "63807  63807  No, non risponderò, come preannunciato. Prefer...   it\n",
       "63808  63808  Ciao, I tecnici della Wikimedia Foundation sta...   it\n",
       "63809  63809  innnazitutto ti ringrazio per i ringraziamenti...   it\n",
       "63810  63810   Kaç olumlu oy gerekiyor? Şu an 7 oldu.  Hayır...   tr\n",
       "63811  63811   Te pido disculpas. La verdad es que no me per...   es\n",
       "\n",
       "[63812 rows x 3 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3989/3989 [==============================] - 505s 127ms/step\n"
     ]
    }
   ],
   "source": [
    "sub['toxic'] = model.predict(test_dataset, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Вот скажите мне дураку! Растолкуйте! ЧТО ЖЕ ВЫ ЗА МУЖИК ТО ТАКОЙ??? Что кроме Ваших модераторов никто неимеет права создавать статьи в Википедии??? Зачем была удалена статья о Алине Кизияровой??? Я хотел ещё раньше создать статью о сериале Дальнобойщики-4 у меня тоже удаляли Ваши коллеги! Наверняка Вам много платят с удовольствием бы поработал как Вы! И Легко и хорошооплачиваемо! Вот это мечта быть на Вашем месте!!! Но Вы мне растолкуете неужели нельзя никому кроме модераторов ничего создавать тут??? Денис-Кошкарёв '"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang = 'ru'\n",
    "test_try = pd.concat((test[test.lang == lang].content, sub.toxic[test.lang == lang]), axis = 1)\n",
    "test_try[test_try.toxic > 0.99].content.values[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19045</th>\n",
       "      <td>Marcus Qwertyus \\n\\n Your a freeking homosexua...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29398</th>\n",
       "      <td>==Please read unblock request carefully\\n\\nThi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40763</th>\n",
       "      <td>Sorry SITUSH, there is no contact us section o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68580</th>\n",
       "      <td>Where do you live, you little shit? \\n\\nGive u...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86945</th>\n",
       "      <td>You seem to have something against me. If you ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89003</th>\n",
       "      <td>Thank you for being such a politically correct...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97667</th>\n",
       "      <td>\"\\n\\nSo I will start with a criticism of the q...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105685</th>\n",
       "      <td>\"\\n\\nElon refers to BFTS as Big FALCON Test St...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120380</th>\n",
       "      <td>\"\\n\\nDiscuss this topic\\nPlease type the follo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121335</th>\n",
       "      <td>You're dead \\n\\nIt won't be hard finding your ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123186</th>\n",
       "      <td>I'm NOT threatening you\\n\\nI'm just joking aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155740</th>\n",
       "      <td>Interested to your address  \\n\\nAm I correct i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181643</th>\n",
       "      <td>::::Give me your address and I'll forward it t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196007</th>\n",
       "      <td>== Please give me your address == \\n\\n So I ca...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205910</th>\n",
       "      <td>\" \\n ::Excuse me for butting in here. I am wha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1140029</th>\n",
       "      <td>Well could you give me your address? I would b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1234777</th>\n",
       "      <td>Give me your address and I'll come over and to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288807</th>\n",
       "      <td>It's a pleasure to watch you post Roy, I can't...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1251387</th>\n",
       "      <td>You should have included your address in the p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1377912</th>\n",
       "      <td>With approval of the city. Maybe you can provi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120271</th>\n",
       "      <td>whew! Forget the phone number. Send me your ad...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827227</th>\n",
       "      <td>The Social Security Administration database ha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398268</th>\n",
       "      <td>My bet was against Mr. Trump's election.  I've...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              comment_text  toxic\n",
       "19045    Marcus Qwertyus \\n\\n Your a freeking homosexua...      1\n",
       "29398    ==Please read unblock request carefully\\n\\nThi...      0\n",
       "40763    Sorry SITUSH, there is no contact us section o...      0\n",
       "68580    Where do you live, you little shit? \\n\\nGive u...      1\n",
       "86945    You seem to have something against me. If you ...      1\n",
       "89003    Thank you for being such a politically correct...      1\n",
       "97667    \"\\n\\nSo I will start with a criticism of the q...      0\n",
       "105685   \"\\n\\nElon refers to BFTS as Big FALCON Test St...      0\n",
       "120380   \"\\n\\nDiscuss this topic\\nPlease type the follo...      0\n",
       "121335   You're dead \\n\\nIt won't be hard finding your ...      1\n",
       "123186   I'm NOT threatening you\\n\\nI'm just joking aro...      0\n",
       "155740   Interested to your address  \\n\\nAm I correct i...      0\n",
       "181643   ::::Give me your address and I'll forward it t...      0\n",
       "196007   == Please give me your address == \\n\\n So I ca...      0\n",
       "205910   \" \\n ::Excuse me for butting in here. I am wha...      0\n",
       "1140029  Well could you give me your address? I would b...      1\n",
       "1234777  Give me your address and I'll come over and to...      1\n",
       "288807   It's a pleasure to watch you post Roy, I can't...      0\n",
       "1251387  You should have included your address in the p...      0\n",
       "1377912  With approval of the city. Maybe you can provi...      0\n",
       "120271   whew! Forget the phone number. Send me your ad...      0\n",
       "827227   The Social Security Administration database ha...      0\n",
       "398268   My bet was against Mr. Trump's election.  I've...      0"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "#train[re.search('not prepared',train.comment_text)]\n",
    "train[train.comment_text.str.contains(\"your address\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[\"I'm not prepared for you, I promise you 13 of January \\n\\n                         /´¯/) \\n                      ,/¯  // \\n                     /    / / \\n             /´¯/'   '/´¯¯`•¸ \\n          /'/   /    /       /¨¯\\\\ \\n        ('(   ´(  ´      ,~/'   ') \\n         \\\\                 \\\\/    / \\n             \\\\           _ •´ \\n            \\\\              ( \\n              \\\\             \\\\\\n\\nVersion 2\\n                         /´¯/) \\n                      ,/¯  // \\n                     /    / / \\n             /´¯/'   '/´¯¯`•¸ \\n          /'/   /    /       /¨¯\\\\ \\n        ('(   ´(  ´      ,~/'   ') \\n         \\\\                 \\\\/    / \\n             \\\\           _ •´ \\n            \\\\              ( \\n              \\\\             \\\\\",\n",
       "        0],\n",
       "       ['My plan A is to see this collection of fools and their bedfellows in the other party dumped and replaced by people who appreciate the needs of working Alaskans in the cities and villages and have a different priority than protecting the Alaskan tax haven and giveaways to the oil companies.',\n",
       "        1]], dtype=object)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[27288].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, lang='en'):\n",
    "    text = str(text)\n",
    "    text = re.sub(r'[0-9\"]', '', text)\n",
    "    text = re.sub(r'#[\\S]+\\b', '', text)\n",
    "    text = re.sub(r'@[\\S]+\\b', '', text)\n",
    "    text = re.sub(r'https?\\S+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[[I'm not prepared for you, I promise you of January \\\\n\\\\n /´¯/) \\\\n ,/¯ // \\\\n / / / \\\\n /´¯/' '/´¯¯`•¸ \\\\n /'/ / / /¨¯\\\\\\\\ \\\\n ('( ´( ´ ,~/' ') \\\\n \\\\\\\\ \\\\\\\\/ / \\\\n \\\\\\\\ _ •´ \\\\n \\\\\\\\ ( \\\\n \\\\\\\\ \\\\\\\\\\\\n\\\\nVersion \\\\n /´¯/) \\\\n ,/¯ // \\\\n / / / \\\\n /´¯/' '/´¯¯`•¸ \\\\n /'/ / / /¨¯\\\\\\\\ \\\\n ('( ´( ´ ,~/' ') \\\\n \\\\\\\\ \\\\\\\\/ / \\\\n \\\\\\\\ _ •´ \\\\n \\\\\\\\ ( \\\\n \\\\\\\\ \\\\\\\\ ] ['My plan A is to see this collection of fools and their bedfellows in the other party dumped and replaced by people who appreciate the needs of working Alaskans in the cities and villages and have a different priority than protecting the Alaskan tax haven and giveaways to the oil companies.' ]]\""
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(train.loc[27288].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}